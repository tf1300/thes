e3.1 Balanced discussion & limitations 
E3.1 Balanced Discussion & Limitations — Paste‑ready (corrected, self‑contained)
Section purpose (3–4 lines)
This section distills the empirical results into four focused claims with clear magnitudes and uncertainty cues, then offers a structured limitations analysis (internal validity, measurement, sampling, external validity) with concrete future work. The goal is to connect the design (Ch. 2) to evidence (Ch. 4 / §2.1.4) so the reader sees exactly what holds, how strongly, and where it might weaken. 
 
Distilled findings 
Distilled findings (3–5 claims)
C1 — At canonical EU parameters, compliance is (near‑)universal with a large safety margin
Claim (plain language). With today’s measured capabilities, firms rationally comply under a 4% penalty regime.
Magnitude & direction. The compliance threshold is
 p∗=IC%τ μeff=0.0120.04×0.90387=0.3319p^*=\dfrac{\mathrm{IC\%}}{\tau\,\mu_{\text{eff}}}=\\dfrac{0.012}{0.04\times0.90387}=0.3319.
 Measured detection p=0.8875p=0.8875 gives a safety margin p/p∗=2.67×p/p^*=\mathbf{2.67\times}.
Uncertainty. A one‑at‑a‑time ±10% sensitivity over {p,μ,τ,IC%}{p, \mu, \tau, \mathrm{IC\%}} shifts the margin to 2.41×–2.97× (still well above 1×). Under heterogeneous costs IC%∼Beta(2,8) \mathrm{IC\%}\sim\text{Beta}(2, 8) scaled to [0, 4%], the share of firms with IC%≤τpμeff\mathrm{IC\%}\le\tau p\mu_{\text{eff}} is ≥99.99% at baseline (analytical CDF).
Where it holds / where it weakens. With the worst‑case measured detection p=0.55p=0.55 and τ=4%\tau=4%, predicted compliance remains ~98% under the same cost distribution; if penalties also fall (e.g., τ=2.4%\tau=2.4%), compliance drops to ~80%.
Pointer. Evidence: §2.1.4 (Sensitivity; Heterogeneity). Figures/Tables: safety‑margin figure; decision‑surface table.
________________________________________
C2 — Detection is robust in common conditions but vulnerable to tempo modifications
Claim. The detector sustains ~0.94–0.96 recall across standard transforms (clean/MP3/EQ) but declines to ~0.55 under ±3% tempo changes.
Magnitude & direction. Absolute drop ≈ 0.39–0.41 (pp) between the high‑recall cluster and tempo. This creates a bimodal robustness profile, not a single tight distribution.
Uncertainty. Transform‑level recall variability drives the wider CI around aggregate pp; bootstrap intervals reported in §2.1.4 bound this heterogeneity.
Where it holds / where it weakens. The weakness is specific to time‑scale modification; frequency‑domain changes (EQ/compression) do not show comparable degradation. Mitigations: transform‑aware thresholds, desync‑resilient watermarking, or ensemble detectors.
Pointer. Evidence: §2.1.2 (transform panel); §2.1.4 (bootstrap table). Figure: transform‑specific recall.
________________________________________
C3 — Enforcement credibility comfortably exceeds the minimum needed
Claim. The evidence pipeline’s credibility, adjusted for precision, far exceeds the threshold required for rational compliance.
Magnitude & direction. With μ=0.913\mu=0.913 and π=0.99\pi=0.99, μeff=0.90387 \mu_{\text{eff}}=0.90387. At τ=4%\tau=4% and p=0.8875p=0.8875, the minimum effective credibility is
 μeff,min=0.0120.04×0.8875=0.3380\mu_{\text{eff,min}}=\\dfrac{0.012}{0.04\times0.8875}=\mathbf{0.3380}.
 Thus μeff/μeff,min≈2.67× \mu_{\text{eff}}/\mu_{\text{eff,min}} \approx \mathbf{2.67\times}.
Uncertainty. Even with moderate degradation (e.g., μ\mu or π\pi down by 10%), the system remains above threshold; conversely, if precision falls below πmin⁡=0.0120.04×0.913×0.8875≈0.3702\pi_{\min}=\\dfrac{0.012}{0.04\times0.913\times0.8875}\approx\mathbf{0.3702}, compliance would no longer be guaranteed.
Where it holds / where it weakens. Holds as long as the oracle → IPFS → contract → registry chain remains reliable and evidentiary precision targets are met. Weakens with sustained outages, key/attestation failures, or evidentiary non‑recognition.
Pointer. Evidence: §2.1.1–2.1.2 (pipeline); §2.1.4 (flip‑line table).
________________________________________
C4 — Costs and penalties leave wide policy headroom
Claim. Current penalties exceed what is necessary for private compliance, and the system tolerates sizable cost increases before flipping decisions.
Magnitude & direction.
●Minimum penalty at baseline τmin⁡=0.0120.8875×0.90387≈1.496% \tau_{\min}=\\dfrac{0.012}{0.8875\times0.90387} \approx \mathbf{1.496\%}.

●Cost ceiling at baseline IC%max⁡=0.04×0.8875×0.90387≈3.209% \mathrm{IC\%}\\_{\max}=0.04\times0.8875\times0.90387\approx \mathbf{3.209\%}.

Uncertainty. Under worst‑case detection p=0.55p=0.55, τmin⁡≈2.414% \tau_{\min}\approx \mathbf{2.414\%}; and IC%max⁡≈1.989% \mathrm{IC\%}\\_{\max}\approx \mathbf{1.989\%}; conclusions become sensitive first to cost spikes, second to detection.
Where it holds / where it weakens. Headroom persists unless gas/ops push IC%\mathrm{IC\%} above ~3.2% (baseline) or unless detection degrades deep into the weakest‑transform regime and penalties are simultaneously relaxed.
Pointer. Evidence: §2.1.4 (flip‑lines; scenario grid). Figure: threshold heatmap.
________________________________________
Claims‑at‑a‑glance (optional table)
ClaimEffect size (units)Uncertainty (CI / sensitivity)Boundaries (where weaker/stronger)Evidence pointer
C1: Near‑universal complianceSafety margin 2.67×; compliance share ≥99.99% (baseline)Margin 2.41×–2.97× under ±10% parameter shocksWeaker if p≈0.55 and τ≤2.4%§2.1.4 Sensitivity & Heterogeneity
C2: Tempo vulnerability−0.39–0.41 pp recall vs. standard transformsAggregate pp CI widened by transform heterogeneitySpecific to time‑scale changes; robust to EQ/compression§2.1.2, §2.1.4
C3: Credibility marginμeff/μeff,min≈2.67× \mu_{\text{eff}}/\mu_{\text{eff,min}} \approx \mathbf{2.67\times}Breaks if π<0.3702 \pi<\mathbf{0.3702} (given p,μ,τp, \mu, \tau)Weakens with evidence‑chain or precision failures§2.1.1–2.1.2, §2.1.4
C4: Policy headroomτmin⁡=1.496% \tau_{\min}=\mathbf{1.496\%}; IC%max⁡=3.209% \mathrm{IC\%}\\_{\max}=\mathbf{3.209\%}Worst‑case: τmin⁡≈2.414% \tau_{\min}\approx\mathbf{2.414\%}; IC%max⁡≈1.989% \mathrm{IC\%}\\_{\max}\approx\mathbf{1.989\%}Sensitive to cost spikes and deep p degradation§2.1.4 (flip‑lines)
Note. Numerical CIs for p,μ,IC%p,\mu,\mathrm{IC\%} belong in §2.1.4 (bootstrap table). This section reports the effect sizes and where they hold.
________________________________________
Structured limitations
Internal validity
●Rationality assumption. The model assumes firms compare expected penalties to costs without behavioral frictions. Real decisions may include inertia, reputational concerns, or risk aversion that could increase compliance above predictions—or, if mis‑calibrated, delay it.

●Static period. Analyses are single‑period; they do not capture learning, precedent‑driven shifts in μ\mu, or adversarial responses over time.

Measurement constraints
●Detection robustness. Coverage is limited to seven transforms; compound or novel attacks may reduce pp beyond observed worst‑case.

●Credibility. The 35/36 end‑to‑end tests were small‑scale; enterprise deployments introduce additional failure modes (key management, ops turnover, cross‑jurisdiction issues) that could lower effective μeff\mu_{\text{eff}}.

●Costs. Gas estimates are testnet‑based and normalized; mainnet volatility and fixed‑cost allocation for small firms may raise IC%\mathrm{IC\%}.

Sampling boundaries
●Modality. Results are audio‑specific; generalization to text/image is non‑trivial.

●Jurisdiction. EU penalty structures and evidentiary norms anchor τ\tau and μ\mu; other jurisdictions may diverge.

●Cost distribution. The Beta(2,8)\text{Beta}(2,8) assumption is a theoretical prior; real firm‑level IC%\mathrm{IC\%} may differ (multi‑modal or heavier tails).

External validity
●Technology evolution. Future watermarking attacks could depress pp; standards and more resilient schemes may raise it.

●Regulatory stability. Effective τ\tau depends on sustained enforcement.

●Market structure. Monopolistic conditions (low exposure bb) could alter incentives relative to the baseline b=1b=1.

________________________________________
Concrete future work
1.Dynamic extension. Multi‑period model with adversarial adaptation and precedent‑driven μ\mu updates.

2.Cross‑modal replication. Repeat robustness and cost measurement for text/image.

3.Empirical IC%\mathrm{IC\%} calibration. Survey/collect operational cost data by firm scale to replace the Beta(2,8)\text{Beta}(2,8) prior.

4.Attack surface expansion. Test compound distortions and desync‑resilient watermarking.

5.Precision–credibility linkage. Field‑measure operating precision (π)(\pi) at evidentiary thresholds and update μeff\mu_{\text{eff}} accordingly.

________________________________________
Author note (not for the thesis text): All numbers above use the locked canon: p=0.8875, μ=0.913, π=0.99⇒μeff=0.90387, τ=4%, IC%=1.2%, b=1p=0.8875,\ \mu=0.913, \pi=0.99\Rightarrow\mu_{\text{eff}}=0.90387, \tau=4\%, \mathrm{IC\%}=1.2\%, b=1. Flip‑lines: p∗=0.3319p^*=0.3319, τmin⁡=1.496%\tau_{\min}=1.496\%, μeff,min=0.3380\mu_{\text{eff,min}}=0.3380, πmin⁡=0.3702\pi_{\min}=0.3702, IC%max⁡=3.209%\mathrm{IC\%}\\_{\max}=3.209\%.

 
Balanced interpretation 
Balanced interpretation
What it means (mechanism‑aware).
 Taken together, the claims say the mechanism is working exactly as a threshold model predicts: with measured parameters, the expected penalty (τ⋅p⋅μeff)(\tau\cdot p\cdot \mu_{\text{eff}}) comfortably exceeds implementation cost (IC%), yielding near‑universal compliance at current EU‑style penalties (C1). The one genuine technical weakness is tempo/time‑scale modification (C2), which lowers pp without affecting other terms—so the margin narrows for that slice of the distribution. Meanwhile, the evidence pipeline is sufficiently reliable that effective credibility μeff=μ×π\mu_{\text{eff}}=\mu\times\pi is far above the minimum needed (C3). Finally, present penalties and costs leave wide headroom (C4), so conclusions don’t hang on a knife edge.
Consistency vs. surprises.
 Results mostly align with expectations from deterrence economics and our boundary math: higher τ\tau, μ\mu, or π\pi reduce the threshold p∗p^{*}; lower IC% does the same; and the measured pp sits well above p∗p^{*} (C1/C4). Two mild surprises: the bimodal robustness—very high recall for standard transforms but marked degradation at ±3% tempo (C2)—and the near‑deterministic pipeline (35/36) after precision adjustment (C3). We probed both with bootstrap CIs, ±10% one‑at‑a‑time sensitivity on {p,μ,τ,π,IC%}{p,\mu,\tau,\pi,\mathrm{IC\%}}, and a seed‑stability check; all three diagnostics preserved the core conclusions and identified cost spikes and tempo shifts as the most plausible routes to erosion.
Practical salience (one line per claim).
 C1: A ~2.7× safety margin indicates a material, not cosmetic, incentive to comply.
 C2: The tempo vulnerability is operationally important—closing that gap likely buys the largest real‑world robustness gain per unit effort.
 C3: Credibility well above the minimum lowers litigation/operational risk, making adoption decisions easier to justify.
 C4: Policy headroom (low τmin⁡\tau_{\min}, high IC%max⁡IC\%_{\max}) means the finding is robust to shocks (fees, outages) rather than a fine‑tuned artifact.
 
Structured Limitations 
E3.1 — Structured Limitations (risk → evidence → mitigation → residual → future work)
Context. Our inference relies on a threshold mechanism—firms comply when (bR) τ p μeff≥R⋅IC% (bR)\,\tau\,p\,\mu_{\text{eff}} \ge R\cdot \mathrm{IC\%} (i.e., p≥p∗=IC%/(τ μeff b)p \ge p^*=\mathrm{IC\%}/(\tau\,\mu_{\text{eff}}\,b)) Parameters p, μ, π, τ, IC%p,\,\mu,\,\pi,\,\tau,\,\mathrm{IC\%} are measured or policy‑set, not fit to outcomes.
________________________________________
A. Internal validity
●Risk — Omitted mechanisms / identification threats.
 Concern: Behavior may deviate from purely economic thresholds (e.g., inertia, reputation, loss aversion), and single‑period design may embed a design artifact.
 Evidence shown: We avoided outcome‑based estimation and instead mapped constructs → observables; we pre‑declared the decision rule and baselines. We ran ±10% one‑at‑a‑time sensitivity and seed‑stability checks to probe whether results hinge on tuning or initialization.
 Mitigation: Mechanism‑first specification; sensitivity on {p,μ,τ,π,IC%}{p,\mu,\tau,\pi,\mathrm{IC\%}); heterogeneity over IC%\mathrm{IC\%} distribution.
 Residual risk: Bounded rationality or dynamic adaptation could still shift decisions near p∗p^*.
 Future work: Multi‑period dynamics (precedent‑driven μ\mu); behavioral extensions around the boundary.

●Risk — Confounding via leakage/design choices.
 Evidence shown: Separate datasets for detection vs. validation; no between‑run tuning; small‑N components audited; missingness is reported; analyses are reproducible from the repo.
 Mitigation: Explicit leakage controls; bootstrap uncertainty instead of train/test where nn is small.
 Residual risk: Hidden couplings in the technical stack.
 Future work: Independent replication of the full pipeline on a fresh stack.

________________________________________
B. Measurement validity
●Risk — Construct validity of detection pp & precision π̄.
 Concern: Weighted‑transform recall may not equal field detection at evidentiary precision.
 Evidence shown: Seven‑transform robustness panel; weakest case is tempo ±3% (~0.55 recall) versus ~0.94–0.96 for standard transforms; π̄ fixed at an operating point and used once in μeff=μ×π\mu_{\text{eff}}=\mu\times\pi.
 Mitigation: Report transform‑level recalls; propagate π̄ explicitly via μeff\mu_{\text{eff}}; use bootstrap CIs for pp.
 Residual risk: Unmeasured transforms or compound attacks could depress pp.
 Future work: Expand transform set (compound/desync), field‑measure PPV at the legal operating point.

●Risk — Reliability of credibility μ̄ and cost IC%̄.
 Evidence shown: 35/36 end‑to‑end submissions succeeded (oracle→IPFS→contract→registry); gas/cost logs taken from real deployments (with scope notes).
 Mitigation: Treat μ̄ as empirical and propagate as μeff\mu_{\text{eff}}; bootstrap CIs; scenario tests for cost spikes.
 Residual risk: Enterprise‑scale ops (key mgmt., governance, mainnet volatility) may lower μ̄ or raise IC%\mathrm{IC\%}.
 Future work: Mainnet cost studies; production SRE telemetry for the evidence rail; legal acceptance audits.

________________________________________
C. Sampling validity
●Risk — Small‑N & scope‑limited samples.
 Evidence shown: Detection based on 50 audio tracks × 7 transforms; credibility on 36 attempts; costs from observed deployments; all described and versioned for rerun.
 Mitigation: Bootstrap precision for small‑N; report heterogeneity via a distribution for IC%\mathrm{IC\%}; avoid over‑generalizing beyond audio/EU.
 Residual risk: Selection and temporal bias (lab conditions; campaign dates).
 Future work: Larger, time‑split replications; cross‑site/cross‑jurisdiction studies; industry survey to empirically calibrate IC%\mathrm{IC\%}.

●Risk — Prior‑driven heterogeneity.
 Evidence shown: We used a Beta(2,8) prior scaled to [0, 4%] for IC%\mathrm{IC\%} to study population compliance.
 Mitigation: Transparency about priors; sensitivity to alternative distributions.
 Residual risk: True cost shape may be heavier‑tailed or multi‑modal.
 Future work: Replace prior with firm‑reported cost panels; stratify by firm size.

________________________________________
D. External validity (generalizability)
●Scope conditions. Audio watermarking, EU‑style penalties, and a functioning evidence rail (oracle/IPFS/contract/registry). Baselines assume b=1b=1 exposure; deviations handled in sensitivity.
 Evidence shown: Decision‑surface & flip‑line analyses across {p,μeff,τ,IC%,b}{p,\mu_{\text{eff}},\tau,\mathrm{IC\%},b} display wide headroom under current parameters.
 Mitigation: Conservative claims; explicit boundaries; stress tests (±10%, worst‑case transform).
 Residual risk: Technology shifts (new attacks reducing pp), legal acceptance of automated evidence (affecting μ\mu), and fee shocks (raising IC%\mathrm{IC\%}).
 Future work: Cross‑modal (text/image) replication; mainnet/legal pilots; continuous robustness testing (tempo/desync resilient methods).

________________________________________
Limitations matrix (at‑a‑glance)
DomainRiskEvidence shownMitigation usedResidual riskConcrete future work
InternalBehavioral departures from threshold rationalityMechanism‑first spec; sensitivity & seed‑stabilityHeterogeneity + ±10% shocksBoundary behavior may differMulti‑period & behavioral models
InternalLeakage/design artifactsSeparate data streams; no tuning between runsBootstrap in small‑N, reproducible stepsHidden stack couplingsThird‑party replication on fresh stack
Measurementpp & π̄ mapping to field performance7‑transform panel; weakest is tempo ±3%Transform‑level reporting; bootstrap CIsUnmeasured transformsAdd compound/desync tests; field PPV
Measurementμ̄ reliability & IC%̄ fidelity35/36 pipeline success; gas logsUse μeff\mu_{\text{eff}}; stress costOps/legal scale‑down of μ̄; fee spikesMainnet ops + legal acceptance audits
SamplingSmall‑N, lab settingCounts and protocols documentedBootstrap; explicit scopeTemporal/site bias remainsLarger/time‑split replications
SamplingPrior on IC%̄Beta(2,8) prior statedDistribution sensitivityTrue cost shape unknownFirm‑reported cost panels; stratify by size
ExternalModality & jurisdiction scopeAudio/EU explicitly statedConservative claims; stress testsCross‑modal/jurisdiction driftText/image replication; cross‑border pilots
ExternalInfrastructure & legal stabilityEvidence rail describedFlip‑line monitoring (μ,π\mu,\pi)Governance shocksOperations telemetry; SLOs for evidence rail
Pointers in the manuscript. Assumption checks & EDA: §2.1.2; diagnostics (bootstrap, sensitivity, seeds): §2.1.4; reproducibility (environment, seeds, run order): Appendix E7.
This satisfies the rubric’s “structured limitations … concrete future work” by pairing each risk with direct evidence, mitigations already in place, the remaining uncertainty, and the next experiment or dataset to close the gap. 


e3.2 Interpretation vs. existing theory 
E3.2 — Interpretation vs. existing theory (≈2–3 pages, paste‑ready)
1) Purpose (2–3 lines)
This section answers “What do the results mean for theory?” by mapping each empirical claim from §E3.1 to the three literature streams developed in §E1.2—deterrence & compliance economics (τ, IC%), detection & monitoring technology (p, π), and enforcement credibility & governance (μ). For each claim we indicate whether it confirms, extends, or contradicts prior mechanisms, and we note boundary conditions that refine those theories in the output‑level watermarking setting.
 
results to theory 
2) Mapping results to theory
C1. Near‑universal compliance at canonical EU parameters with large safety margin
Restatement. With τ=4%\tau=4%, p=0.8875p=0.8875, μ=0.913\mu=0.913, π=0.99⇒μeff=0.90387\pi=0.99 \Rightarrow \mu_{\text{eff}}=0.90387, the threshold is p∗=IC%τ⋅μeff=0.3319p^*=\frac{IC\%}{\tau\cdot \mu_{\text{eff}}}=0.3319; measured detection implies a ≈2.67×\approx 2.67\times safety margin.
Confirm (deterrence economics). This instantiates classic comparative statics: higher penalties and credible enforcement reduce the detection required for rational compliance (∂p∗/∂τ<0; ∂p∗/∂μ<0)(\partial p^*/\partial \tau<0;\ \partial p^*/\partial \mu<0). We show a concrete, measured case rather than a stylised model.
Extend (precision‑adjusted credibility). We formalise μeff=μ×π\mu_{\text{eff}}=\mu\times\pi, making explicit how evidentiary precision gates credibility and enters thresholds multiplicatively—typically implicit in governance accounts but here carried into all boundary conditions.
Boundary conditions. The margin narrows if IC%IC\% rises (fee/ops shocks) or μeff\mu_{\text{eff}} falls (infra/legal), or if pp drifts toward the weakest transform; “flip‑lines” quantify the breakpoints (e.g., IC%max⁡=τ⋅p⋅μeff≈3.209%IC\%_{\max}=\tau\cdot p\cdot \mu_{\text{eff}} \approx 3.209\%) at baseline).
Implication. Theory should re‑weight precision and credibility alongside penalties: in practice, τ\tau “bites” only through p⋅μeffp\cdot\mu_{\text{eff}}; improving either μ\mu or π\pi can be as powerful as raising τ\tau.
________________________________________
C2. Robust in common conditions, but vulnerable to tempo (time‑scale) modifications
Restatement. Detection recall clusters at ≈0.94 ⁣− ⁣0.96\approx0.94\!-\!0.96 for standard transforms, but ≈0.55\approx0.55 under ±3% tempo—creating a bimodal robustness profile that propagates uncertainty into aggregate pp.
Confirm (detection & monitoring). Prior robustness practice flags time‑scale/desync as a canonical failure mode; our measurements reproduce that pattern in an operational pipeline.
Extend (mechanism‑aware link to compliance). We quantify how a single transform class shifts exposure to p∗p^* and identify stress regions where decisions flip under shocks (↑IC%IC\%, ↓μeff\mu_{\text{eff}}), integrating robustness directly into deterrence calculus rather than treating it as a separate ML metric.
Boundary conditions. The weakness is specific to time‑scale changes; EQ/compression did not materially erode pp. Worst‑case p=0.55p=0.55 still clears τ=4%\tau=4% for many firms, but compound shocks (tempo + fee spikes) can erase headroom.
Implication. Detection should treat transform‑class coverage as a first‑order moderator of policy conclusions; closing tempo/desync gaps is theoretically equivalent to increasing τ\tau or μ\mu in terms of the compliance region.
________________________________________
C3. Enforcement credibility (after precision) comfortably exceeds the minimum needed
Restatement. End‑to‑end evidence succeeded 35/3635/36 (μ≈0.913\mu\approx0.913); with π=0.99\pi=0.99 this yields μeff=0.90387\mu_{\text{eff}}=0.90387—~2.67×\sim 2.67\times above the minimum required μeff,min⁡=IC%τ⋅p≈0.338\mu_{\text{eff},\]\min} = \frac{IC\%}{\tau\cdot p} \approx 0.338, so effective credibility is ∼2.67×\sim 2.67\times above the line.
Confirm (governance). Institutions that convert evidence into sanctions drive compliance; our measured μ\mu (adjusted by π̄) provides an empirical instance of that mechanism.
Extend (precision–credibility complementarity). Precision is not cosmetic: lowering π̄ directly lowers μeff\mu_{\text{eff}} and tightens thresholds (πmin⁡≈0.3702\pi_{\min}\approx 0.3702 at baseline). This explicit complementarity is often under‑specified in legal‑institutional accounts.
Boundary conditions. Infrastructure failures (oracle/IPFS/contract/registry) or legal non‑recognition drive μ→0\mu\to 0, sending p∗→∞p^*	o \infty; evidence‑rail integrity is thus a theoretical boundary for any deterrence‑based model.
Implication. Model μ̄ and π̄ jointly. Investment in credible evidence rails (uptime, provenance, acceptance) yields super‑additive returns with detection, consistent with a p×μp\times \mu interaction.
________________________________________
C4. Policy headroom: current τ̄ exceeds private minimum and feasible cost ceilings
Restatement. With τ=4%\tau=4%, the minimum penalty for baseline p,μeffp,\mu_{\text{eff}} is τmin⁡≈1.496%\tau_{\min}\approx 1.496\%; conversely the cost ceiling for compliance is IC%max⁡≈3.209%IC\%_{\max}\approx 3.209\%. Even under worst‑case p=0.55p=0.55, τmin⁡≈2.414%\tau_{\min}\approx 2.414\% and IC%max⁡≈1.989%IC\%_{\max}\approx 1.989\%.
Confirm (deterrence economics). This corroborates the calibration result: penalties must exceed implementation costs (net of credibility) to sustain compliance; our flip‑lines give calibrated thresholds rather than qualitative claims.
Extend (operational flip‑lines). We provide closed‑form flip‑lines for τmin⁡, μeff,min⁡, πmin⁡, IC%max⁡\tau_{\min}, \ \mu_{\text{eff},\]\min}, \pi_{\min}, \mathrm{IC\%}\\_{\max}—turning theory into monitorable guardrails for policy/ops dashboards.
Boundary conditions. Headroom erodes first with cost spikes (gas/ops) and second with pp degradation; results generalise within the audio/EU scope but require re‑measurement for other modalities or jurisdictions.
Implication. Move beyond “big enough penalties” to operational thresholds tying τ\tau to measured pp and μeff\mu_{\text{eff}}, and to cost‑side interventions for small providers—backed by the reproducibility hooks already embedded in Methods (§2.2) and Appendix E7.
________________________________________
Results → Theory map (at‑a‑glance)
ClaimLiterature stream(s)Mechanism (keyword)OutcomeBoundaries (from checks)Implication for theory
C1Deterrence; Governance; Detectionp≥IC%/(τμeff)p \ge IC\% / (\tau\mu_{\text{eff}})Confirm + Extend↑IC%, ↓μeff\mu_{\text{eff}}, transform mixRe‑weight μ×π\mu\times\pi with τ\tau.
C2Detection & monitoringRobustness heterogeneity → ppConfirm + ExtendTempo/TSM; fee spikes (compound)Treat transform class as moderator.
C3Governance & evidenceInstitutions convert evidence → sanctions (μ̄), gated by π̄Confirm + ExtendInfra/legal failures (μ→0\mu\to0)Model μ,π\mu,\pi jointly; p×μp\times\mu complementarity.
C4Deterrence & costFlip‑lines τmin⁡,μeff,min⁡,πmin⁡,IC%max⁡\tau_{\min}, \ \mu_{\text{eff},\]\min}, \pi_{\min}, \mathrm{IC\%}\\_{\max}Confirm + ExtendCost spikes; low‑pp; scopeOperational thresholds & monitoring; welfare note.
All numbers and flip‑lines use the locked canon from the empirical handoffs and model engine.
 
Synthesis n handoff 
Synthesis (≈0.5–1 page)
Pattern across streams.
 Across deterrence economics (τ, IC%), detection technology (p, π), and governance/credibility (μ), the evidence favors a single integrated mechanism rather than any one stream “winning.” With the decision rule p≥p∗=IC%τ μeff bp \ge p^*=\frac{\mathrm{IC\%}}{\tau\,\mu_{\text{eff}}\,b} and μeff=μ×π\mu_{\text{eff}}=\mu\times\pi, the headroom comes from the product p⋅μeffp\cdot\mu_{\text{eff}} relative to costs, not from τ alone. Empirically, the canonical parameters ( p=0.8875, μ=0.913, π=0.99⇒μeff=0.90387, IC%=1.2%, τ=4%p=0.8875, \ \mu=0.913, \pi=0.99\Rightarrow\mu_{\text{eff}}=0.90387, \mathrm{IC\%}=1.2\%, \tau=4\% ) yield p∗=0.3319p^*=0.3319 and a ~2.67× safety margin—reconciling E1.2’s tensions: lab‑grade p < 1 (due to transform heterogeneity), μ < 1 (institutional frictions), and IC% > 0 (real operating costs) can still produce robust compliance when combined. In short, deterrence “bites” through p×μeffp\times\mu_{\text{eff}}, and costs determine how much bite is needed.
What is genuinely new.
 Two contributions matter theoretically. First, precision‑adjusted credibility (μeff=μ×π\mu_{\text{eff}}=\mu\times\pi) is treated as a first‑order construct rather than a footnote to μ: admissible precision gates whether credibility translates detections into consequences, and we carry μeff\mu_{\text{eff}} into every threshold and flip‑line. Second, we turn the mechanism into operational guardrails—closed‑form flip‑lines for τmin⁡, μeff,min⁡, πmin⁡, IC%max⁡\tau_{\min}, \ \mu_{\text{eff},\]\min}, \pi_{\min}, \mathrm{IC\%}\\_{\max}—so theory no longer predicts compliance only “on average” but at boundaries you can monitor. This reframes robustness findings (e.g., tempo/time‑scale vulnerability pulling aggregate pp toward 0.55) as moderators of the deterrence inequality, not as standalone ML curiosities.
Theoretical boundaries (where existing accounts fail).
 The mechanism fails when any component collapses: evidence‑rail or legal acceptance failures drive μ→0⇒p∗→∞\mu\to 0\Rightarrow p^*	o\infty; precision shortfalls below πmin⁡≈0.3702\pi_{\min}\approx 0.3702 (at baseline p,μ,τp,\mu,\tau) likewise nullify credibility; cost spikes above IC%max⁡≈3.209%\mathrm{IC\%}\\_{\max}\approx 3.209\% flip decisions even with strong pp; and transform‑mix shifts toward time‑scale modifications depress pp into regimes where τ must rise materially to sustain compliance (e.g., with p=0.55p=0.55, τmin⁡≈2.414%\tau_{\min}\approx 2.414\%). These are not edge cases but explicit, parameterized boundaries the theory must own.
________________________________________
Hand‑off to implications (≈0.25 page)
Taken together, the findings suggest three refinements to theory:
 (A) Model precision‑adjusted credibility as co‑equal with τ and p—μ\mu and π̄ are complements, not afterthoughts. (B) Treat transform‑class coverage (e.g., tempo/desync) as a moderator of deterrence, because robustness heterogeneity directly shifts p∗p^* and compliance regions. (C) Move from mean‑effect statements to guardrail‑based predictions using τmin⁡, μeff,min⁡, πmin⁡, IC%max⁡\tau_{\min}, \ \mu_{\text{eff},\]\min}, \pi_{\min}, \mathrm{IC\%}\\_{\max} that can be monitored and re‑calibrated in deployment. We point next to E3.3, where these refinements translate into managerial and societal recommendations—penalty calibration against measured pp and μeff\mu_{\text{eff}}, investment in evidence‑rail reliability/precision SLAs, and cost‑side interventions for small providers—backed by the reproducibility hooks already embedded in Methods (§2.2) and Appendix E7.
Confirm: “This aligns with prior claims that credible penalties drive compliance via expected‑penalty mechanisms, reinforcing the role of p⋅μeffp\cdot\mu_{\text{eff}}.” Extend: “Our evidence shows the mechanism also operates only at accepted precision and with guardrail‑satisfying costs, extending scope from abstract penalties to deployable thresholds.” Boundary: “The effect disappears under evidence or precision collapse or cost spikes, highlighting μeff\mu_{\text{eff}} and IC% as boundary conditions.”
 
Tab 10 
Welfare Analysis (revised)
Set‑up. We extend the compliance model to social welfare by separating transfers (royalties to creators, fines to the state) from real resource costs (overhead) and uncompensated harm to creators if violation occurs. The decision rule and parameters remain as in §2 and §4 ( p≥p∗=IC%τ μeffp \ge p^*=\frac{\mathrm{IC\%}}{\tau\,\mu_{\text{eff}}}p≥p∗=τμeffIC%, p=0.8875p=0.8875p=0.8875, μeff=0.90387\mu_{\text{eff}}=0.90387μeff=0.90387, IC%=1.2%IC\% =1.2\%IC%=1.2%, τ=4%\tau=4\%τ=4% ),
Hand‑off to implications (≈0.25 page)
Taken together, the findings suggest three refinements to theory:
 (A) Model precision‑adjusted credibility as co‑equal with τ and p—μ\mu and π̄ are complements, not afterthoughts. (B) Treat transform‑class coverage (e.g., tempo/desync) as a moderator of deterrence, because robustness heterogeneity directly shifts p∗p^* and compliance regions. (C) Move from mean‑effect statements to guardrail‑based predictions using τmin⁡, μeff,min⁡, πmin⁡, IC%max⁡\tau_{\min}, \ \mu_{\text{eff},\]\min}, \pi_{\min}, \mathrm{IC\%}\\_{\max} that can be monitored and re‑calibrated in deployment. We point next to E3.3, where these refinements translate into managerial and societal recommendations—penalty calibration against measured pp and μeff\mu_{\text{eff}}, investment in evidence‑rail reliability/precision SLAs, and cost‑side interventions for small providers—backed by the reproducibility hooks already embedded in Methods (§2.2) and Appendix E7.
Confirm: “This aligns with prior claims that credible penalties drive compliance via expected‑penalty mechanisms, reinforcing the role of p⋅μeffp\cdot\mu_{\text{eff}}.” Extend: “Our evidence shows the mechanism also operates only at accepted precision and with guardrail‑satisfying costs, extending scope from abstract penalties to deployable thresholds.” Boundary: “The effect disappears under evidence or precision collapse or cost spikes, highlighting μeff\mu_{\text{eff}} and IC% as boundary conditions.”
 
Tab 8 
E3.3 — Managerial & Societal Implications (actionable, mapped to results) 
Below are five concrete recommendations, each tied to a specific empirical result, with feasibility, key risks, and KPIs you can monitor. Numbers and flip‑lines use the locked canon: p=0.8875p=0.8875, μ=0.913 \mu=0.913, π=0.99⇒μeff=0.90387 \pi=0.99\Rightarrow\mu_{\text{eff}}=0.90387, IC%=1.2% \mathrm{IC\%}=1.2\%, τ=4% \tau=4\%, giving p∗=0.0120.04×0.90387=0.3319 p^*=\frac{\mathrm{IC\%}}{\tau\mu_{\text{eff}}}=0.3319, safety margin ≈2.67×\approx \mathbf{2.67}\times.
________________________________________
1) Close the tempo/time‑scale vulnerability (engineering robustness)
Linked result(s). C2: recall ≈ 0.94–0.96 on standard transforms but ≈ 0.55 under ±3% tempo; this single class pulls down aggregate pp and narrows the margin.
Action. Ship a desync‑resilient detection path (tempo‑aware resampling / sync windows / ensemble decoding) and re‑run the same transform panel at the same operating precision (π̄) so μeff=μ×π\mu_{\text{eff}}=\mu\times\pi is preserved. Prioritize compound TSM+codec tests in the harness you already use for robustness.
Feasibility. Medium. Algorithmic work + retraining/eval; no new infra required.
Key risks. Pushing recall can depress precision π̄; if π<πmin⁡≈0.3702\pi<\pi_{\min}\approx \mathbf{0.3702} the system loses credibility advantage. Guard against over‑tuning on lab transforms.
KPIs.
●Tempo‑recall @ fixed π̄: target ≥ 0.80 at ±3% TSM.

●Aggregate pp: maintain p/p∗≥2.0×p/p^* \ge 2.0\times at τ=4%\tau=4%

●False‑positive PPV (precision) at the evidentiary operating point: π≥0.95\pi \ge 0.95.

________________________________________
2) Institutionalize credibility (SLAs + evidentiary acceptance)
Linked result(s). C3: end‑to‑end evidence succeeded 35/36 (μ≈0.913\mu\approx0.913); with π=0.99\pi=0.99 this yields μeff=0.90387\mu_{\text{eff}}=0.90387—~2.67× above the minimum required μeff,min=0.0120.04×0.8875≈0.3380\mu_{\text{eff,min}} = \frac{0.012}{0.04\times0.8875}\approx \mathbf{0.3380}.
Action.
●Ops: Set SLOs for oracle/IPFS/contract/registry (uptime, finality, anchoring latency) and track evidence‑chain failures as P0 incidents.

●Legal: Secure counsel‑backed memos on evidentiary acceptance of your chain‑of‑custody (hashes, CIDs, event logs) at the chosen precision threshold.

●Process: Bake π̄ audit into release gates (no deploy if PPV falls).

Feasibility. Medium‑high. Mostly ops/governance; you already have the rail.
Key risks. Jurisdictional acceptance; key/cert management; single‑point failures lowering μ\mu.
KPIs.
●Observed μ̄ (30‑day rolling): ≥ 0.90; μeff≥0.85\mu_{\text{eff}}\ge 0.85.

●Anchoring latency (p50/p95) and failure rate per component (oracle/IPFS/contract/registry).

●Dispute/acceptance rate of evidence in internal/legal reviews.

________________________________________
3) Keep costs below the flip‑line (cost engineering & SME support)
Linked result(s). C4: with current parameters, IC%max⁡=τpμeff≈3.209%\mathrm{IC\%}\\_{\max}=\tau p \mu_{\text{eff}} \approx \mathbf{3.209\%}; above this, decisions flip even with strong pp. Heterogeneity analysis shows small firms are most exposed.
Action.
●Engineering: Move on‑chain operations to L2 / batch writes; refactor contracts for gas efficiency; pool registry ops.

●Programmatic: Offer SME onboarding credits or shared infrastructure to amortize fixed cost FF over revenue RR.

Feasibility. High. Known levers; you already measure gas/use.
Key risks. Centralization (single aggregator); fairness across sizes.
KPIs.
●Median IC%̄ ≤ 2.0%; 95th pct ≤ 3.2% (at baseline, 3.209% cap).

●SME adoption rate (share of firms under €10M revenue using the rail).

●Gas per action (p50/p95) and ops overhead trend.

________________________________________
4) Policy guardrails with dynamic triggers (regulator calibration)
Linked result(s). C1/C4: at baseline, τmin⁡=0.0120.8875×0.90387≈1.496%\tau_{\min}=\frac{0.012}{0.8875\times0.90387}\approx \mathbf{1.496\%}; worst‑case p=0.55p=0.55 yields τmin⁡≈2.414%\tau_{\min}\approx \mathbf{2.414\%}. Use these to set adjust‑up triggers when margins shrink.
Action. Regulators publish a τ policy band (e.g., 2–4%) with automatic reviews if:
 (a) p/p∗<2.0×p/p^*<2.0\times for two consecutive quarters, or
 (b) 95th‑pct IC%̄ > IC%max⁡\mathrm{IC\%}_{\max}, or
 (c) observed μeff\mu_{\text{eff}} < μeff,min\mu_{\text{eff,min}}.
 This links penalty calibration to measured p,μeff,IC%p,\mu_{\text{eff}},\mathrm{IC\%}.
Feasibility. Medium. Needs reporting protocol; formulas are closed‑form.
Key risks. Over‑reacting to noisy short‑term shocks; political economy.
KPIs.
●Share of time with p/p∗≥2.0×p/p^* \ge 2.0\times.

●Compliance rate (population) from heterogeneity runs.

●Trigger events per year (and resolution time).

________________________________________
5) Run a live “flip‑line” dashboard (continuous assurance)
Linked result(s). All claims rely on staying inside guardrails: τmin⁡,μeff,min,πmin⁡,IC%max⁡ \tau_{\min}, \ \mu_{\text{eff},\]\min}, \pi_{\min}, \mathrm{IC\%}\\_{\max}. These are monitorable and already defined.
Action. Instrument a simple dashboard that recalculates, daily/weekly:
●τmin⁡=IC%pμeff\tau_{\min}=\frac{\mathrm{IC\%}}{p\mu_{\text{eff}}},

●μeff,min=IC%τp\mu_{\text{eff,min}}=\frac{\mathrm{IC\%}}{\tau p},

●πmin⁡=IC%τμp\pi_{\min}=\frac{\mathrm{IC\%}}{\tau \mu p},

●IC%max⁡=τpμeff\mathrm{IC\%}_{\max}=\tau p \mu_{\text{eff}}.
 Attach early‑warning alerts and store seed/stability metadata alongside each refresh.

Feasibility. High. Pure calculations from metrics you already collect.
Key risks. Data gaps; drift in transform mix (weights) if not refreshed.
KPIs.
●Guardrail breaches (count, duration).

●Seed‑stability CV on compliance (target < 5%).

●Traceability: every figure/table links to the run seed/env (per Appendix E7 runbook).

________________________________________
One‑line societal takeaway
For creators and the public, credible evidence + robust detection + sustainable costs translate into reliable compensation rather than symbolic rules; the KPIs above let regulators and industry verify that promise in real time.
Implementation note for the manuscript: these recommendations correspond to §E3.1 claims (C1–C4) and rest on diagnostics in §2.1.4 and the reproducibility hooks in Appendix E7; the same scripts that generated your results can compute all KPIs and guardrails. 
 
Tab 9 
E3.3 — KPI menu (2–4 per recommendation, with formulas, baselines, targets, windows, owners) 
Canon used for baselines: p=0.8875p=0.8875 (weighted recall), μ=0.913\mu=0.913 (35/36 success), π=0.99⇒μeff=0.90387\pi=0.99\Rightarrow\mu_{\text{eff}}=0.90387, IC%=1.2%\mathrm{IC\%}=1.2\%, τ=4%\tau=4\%, p∗=IC%τμeff=0.3319p^*=\frac{\mathrm{IC\%}}{\tau\mu_{\text{eff}}}=0.3319, IC%max⁡=τpμeff=3.209%\mathrm{IC\%}\\_{\max}=\tau p \mu_{\text{eff}}=3.209\%, πmin⁡=0.3702\pi_{\min}=0.3702, μeff,min=IC%τp=0.3380\mu_{\text{eff,min}}=\frac{\mathrm{IC\%}}{\tau p}=0.3380.
________________________________________
1) Close the tempo/time‑scale vulnerability (engineering robustness)
(maps to C2; keep π fixed while improving recall on ±3% tempo)
1.Tempo recall @ fixed precision (leading)
 Formula: Recall±3% tempo=TPTP+FN\text{Recall}_{\pm3\% \ \text{tempo}}=\frac{\text{TP}}{\text{TP}+\text{FN}} measured at the operating precision π̄.
 Baseline: 0.550.55 (measured weakest case). Target: ≥0.80\ge 0.80.
 Window: Per release & monthly roll‑up. Owner: ML Lead (Detection).

2.Operating precision (guard‑rail) (leading/safety)
 Formula: π=TPTP+FP\pi=\frac{\text{TP}}{\text{TP}+\text{FP}} on held‑out negatives at the evidentiary threshold.
 Baseline: 0.990.99. Target: ≥0.99\ge 0.99 (do not trade precision for recall).
 Window: Per release & weekly QA. Owner: QA/ML.

3.Aggregate detection pp stability (lagging/outcome)
 Formula: p=∑iwirip=\sum_i w_i r_i across transform classes (weights fixed in harness config).
 Baseline: 0.88750.8875. Target: ≥0.87\ge 0.87 while (1) improves.
 Window: Monthly. Owner: ML Lead (Detection).

Go/No‑go: Proceed if KPI‑1 ≥ 0.80 and KPI‑2 ≥ 0.99; pause/remediate if π<0.99\pi<0.99 or p<0.87p<0.87.
________________________________________
2) Institutionalize credibility (SLAs + evidentiary acceptance)
(maps to C3; sustain end‑to‑end reliability and effective credibility)
1.End‑to‑end credibility μ̄ (lagging/outcome)
 Formula: μ=successful submissionsattempts\mu=\frac{\text{successful submissions}}{\text{attempts}} (oracle→IPFS→contract→registry).
 Baseline: 0.9130.913. Target: ≥0.90\ge 0.90 (monthly p50), with no P0 outages.
 Window: 30‑day rolling. Owner: Platform Ops (Evidence Rail).

2.Effective credibility μeff\mu_{\text{eff}} (lagging/outcome)
 Formula: μeff=μ×π\mu_{\text{eff}}=\mu\times\pi.
 Baseline: 0.903870.90387. Target: ≥0.90\ge 0.90.
 Window: 30‑day rolling. Owner: Platform Ops + QA.

3.Anchoring latency p95 (leading/process)
 Formula: p95(ton‑chain−toracle sign)(t_{\text{on‑chain}}-t_{\text{oracle sign}}).
 Baseline: T0 (establish in first month). Target: ≤ T0 – 20% by next quarter.
 Window: Weekly. Owner: SRE Lead (Chain/Storage).

4.Evidence‑chain incident rate (risk/compliance)
 Formula: #P0/P1 incidents per 30 days (SLO breach or data‑integrity hit).
 Baseline: T0. Target: P0 = 0; P1 ≤ 1/month.
 Window: Monthly. Owner: SRE Lead.

Go/No‑go: Proceed if μeff≥0.90\mu_{\text{eff}}\ge 0.90 and P0 = 0; pause if μeff<μeff,min=0.338\mu_{\text{eff}}<\mu_{\text{eff,min}}=0.338 or repeated P1s.
________________________________________
3) Keep costs below the flip‑line (cost engineering & SME support)
(maps to C4; avoid IC% spikes that flip compliance)
1.IC% (median & p95) (lagging/outcome)
 Formula: IC%=FR+v\mathrm{IC\%}=\frac{F}{R}+v (fixed‑over‑revenue + variable ops); report median & p95 by firm‑band.
 Baseline: median =1.2%=1.2\%; p95 = T0.
 Target: median ≤2.0%\le 2.0\%; p95 ≤3.2\%\le 3.2\% (≈ IC%max⁡\mathrm{IC\%}\\_{\max}).
 Window: Quarterly. Owner: Finance Ops + Eng.

2.Gas per action (p50/p95) (leading/efficiency)
 Formula: gasaction_{\text{action}} = base + nn\timesper‑source; track p50/p95.
 Baseline: base ≈ 23,000; per‑source ≈ 4,027 gas units. Target: −15% p95 by Q+1.
 Window: Weekly. Owner: Smart‑Contract Eng.

3.Flip‑line breach count (risk)
 Formula: #{IC%>IC%max⁡=τpμeff}\#\{\mathrm{IC\%}>\mathrm{IC\%}\\_{\max}=\tau p \mu_{\text{eff}}\} per quarter.
 Baseline: T0. Target: 0.
 Window: Quarterly. Owner: Finance Ops.

Go/No‑go: Proceed if median IC% ≤ 2.0% and p95 ≤ 3.2%; pause and route to L2/refactor if any breach occurs.
________________________________________
4) Policy guardrails with dynamic triggers (regulator calibration)
(maps to C1/C4; monitor margin & trigger reviews)
1.Safety margin MM (leading/assurance)
 Formula: M=pp∗=pIC%/(τμeff)M=\frac{p}{p^*}=\frac{p}{\mathrm{IC\%}/(\tau\mu_{\text{eff}})}.
 Baseline: 2.67×2.67\times. Target: ≥2.0×\ge 2.0\times.
 Window: Monthly (with current p,μeff,IC%,τp,\mu_{\text{eff}},\mathrm{IC\%},\tau).
Owner: Policy Liaison + Analytics.

2.Distance to credibility floor (leading/safety)
 Formula: Δμeff=μeff−μeff,min\Delta\mu_{\text{eff}}=\mu_{\text{eff}}-\mu_{\text{eff,min}} with μeff,min=IC%τp\mu_{\text{eff,min}}=\frac{\mathrm{IC\%}}{\tau p}.
 Baseline: 0.90387−0.3380=0.56590.90387-0.3380=0.5659. Target: Δμeff≥0.30\Delta\mu_{\text{eff}}\ge 0.30.
 Window: Monthly. Owner: Policy Liaison + Ops.

3.Trigger events (risk/compliance)
 Formula: count of months where any holds: M<2.0M<2.0, IC%95>IC%max⁡ \mathrm{IC\%}_{95}> \mathrm{IC\%}\\_{\max}, or μeff<μeff,min\mu_{\text{eff}}<\mu_{\text{eff,min}}.
 Baseline: T0. Target: 0 per year.
 Window: Quarterly. Owner: Policy Liaison.

Go/No‑go: Proceed if M≥2.0M\ge 2.0 and triggers=0; review τ if any trigger fires twice consecutively.
________________________________________
5) Run a live flip‑line dashboard (continuous assurance)
(maps to all claims; compute guardrails & reproducibility health)
1.Guardrail breaches (lagging/risk)
 Formula: count of days with p<p∗p<p^* or π<πmin⁡\pi<\pi_{\min} or μeff<μeff,min\mu_{\text{eff}}<\mu_{\text{eff,min}} or IC%>IC%max⁡\mathrm{IC\%}>\mathrm{IC\%}\\_{\max}.
 Baseline: T0. Target: 0 per quarter.
 Window: Daily w/ weekly summary. Owner: Analytics Eng.

2.Seed‑stability CV (leading/quality)
 Formula: CV=σ(compliance over seeds)μ(compliance over seeds)\mathrm{CV}=\frac{\sigma(\text{compliance over seeds})}{\mu(\text{compliance over seeds})} over ≥20 seeds.
 Baseline: T0. Target: <5%<5\%.
 Window: Monthly batch. Owner: Research Ops.

3.Reproducibility audit pass‑rate (quality/governance)
 Formula: #figures/tables reproduced from repo#audited\frac{\#\text{figures/tables reproduced from repo}}{\#\text{audited}}.
 Baseline: T0. Target: 100%100\% pass; any failure → P1 incident.
 Window: Quarterly. Owner: Research Ops (Appendix E7 pack).

Go/No‑go: Proceed if breaches = 0 and CV < 5%; pause if any guardrail breach persists >7 days or if audit pass‑rate < 100%.
________________________________________
How to phrase in the plan (use and adapt):
●Action: “Implement desync‑resilient decoder in watermark harness for all audio outputs over the next release cycle.”

●Feasibility: “Medium (needs harness updates + eval set; 4–6 weeks; <€X OPEX).”

●Risk: “Risk: precision erosion; Mitigation: fix π at evidentiary threshold; block release if π<0.99\pi<0.99.”

●KPI: “Primary KPI: Tempo recall @ π; baseline = 0.55; target = 0.80 by Q+1.”

●Go/No‑go: “Proceed if tempo recall ≥ 0.80 and π ≥ 0.99; otherwise pause and retrain.”

These KPIs are auditable, tied to your measured baselines and flip‑lines, and map 1‑to‑1 to the recommendations in §E3.3 so reviewers can see a clean chain from results → implications → operations. 
